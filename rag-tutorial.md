---
layout: default
title: RAG-tutorial
---

# Tutorials on RAG

Details on RAG experiments.

<a href="https://github.com/PrabuAppDev/genai-rag/blob/main/rag-101.md" target="_blank">View the latest RAG tutorial on GitHub</a>

```python
!pip install transformers datasets faiss-cpu sentence-transformers
```

    Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.12/site-packages (4.45.2)
    Requirement already satisfied: datasets in /opt/anaconda3/lib/python3.12/site-packages (3.0.1)
    Requirement already satisfied: faiss-cpu in /opt/anaconda3/lib/python3.12/site-packages (1.9.0)
    Requirement already satisfied: sentence-transformers in /opt/anaconda3/lib/python3.12/site-packages (3.1.1)
    Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)
    Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.25.1)
    Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)
    Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (23.2)
    Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.1)
    Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2023.10.3)
    Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.2)
    Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.4.5)
    Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.20.0)
    Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (4.66.4)
    Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (17.0.0)
    Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.3.8)
    Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (2.2.2)
    Requirement already satisfied: xxhash in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.5.0)
    Requirement already satisfied: multiprocess in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.70.16)
    Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.3.1)
    Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.9.5)
    Requirement already satisfied: torch>=1.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (2.4.1)
    Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (1.4.2)
    Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (1.13.1)
    Requirement already satisfied: Pillow in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (10.3.0)
    Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.2.0)
    Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (23.1.0)
    Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.0)
    Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (6.0.4)
    Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.9.3)
    Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)
    Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.0.4)
    Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)
    Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.2.2)
    Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2024.6.2)
    Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.12)
    Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)
    Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)
    Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (69.5.1)
    Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)
    Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2024.1)
    Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2023.3)
    Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)
    Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (2.2.0)
    Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)
    Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)
    Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.12/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)
    


```python
from transformers import GPT2LMHeadModel, GPT2Tokenizer
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import torch
import requests
import PyPDF2
```


```python
from transformers import GPTNeoForCausalLM, GPT2Tokenizer

# Load a smaller GPT-Neo model
model_name = "EleutherAI/gpt-neo-125M"
model = GPTNeoForCausalLM.from_pretrained(model_name)
tokenizer = GPT2Tokenizer.from_pretrained(model_name)

# Set the device to CPU or GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)
model.eval()
```




    GPTNeoForCausalLM(
      (transformer): GPTNeoModel(
        (wte): Embedding(50257, 768)
        (wpe): Embedding(2048, 768)
        (drop): Dropout(p=0.0, inplace=False)
        (h): ModuleList(
          (0-11): 12 x GPTNeoBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): GPTNeoAttention(
              (attention): GPTNeoSelfAttention(
                (attn_dropout): Dropout(p=0.0, inplace=False)
                (resid_dropout): Dropout(p=0.0, inplace=False)
                (k_proj): Linear(in_features=768, out_features=768, bias=False)
                (v_proj): Linear(in_features=768, out_features=768, bias=False)
                (q_proj): Linear(in_features=768, out_features=768, bias=False)
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): GPTNeoMLP(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              (act): NewGELUActivation()
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (lm_head): Linear(in_features=768, out_features=50257, bias=False)
    )




```python
# Load a Sentence Transformer model to convert sentences to embeddings
embedder = SentenceTransformer('all-MiniLM-L6-v2')

# Dummy corpus of documents for retrieval
documents = [
    "The Eiffel Tower is located in Paris.",
    "The Great Wall of China is visible from space."
    # "Artificial Intelligence is transforming the world.",
    # "The Grand Canyon is one of the seven wonders of the world."
]

# Convert the documents into embeddings
document_embeddings = embedder.encode(documents)

# Initialize FAISS index
dimension = document_embeddings.shape[1]  # Dimension of the embeddings
faiss_index = faiss.IndexFlatL2(dimension)  # L2 distance index
faiss_index.add(np.array(document_embeddings))  # Add document embeddings to index

```


```python
def generate_without_rag(query):
    """Generate a response from the LLM without any external document retrieval."""
    # Tokenize the query and generate response
    inputs = tokenizer(query, return_tensors="pt").to(device)

    # Generate output without any retrieved document
    outputs = model.generate(
        inputs['input_ids'],
        attention_mask=inputs['attention_mask'],
        max_length=50,
        num_return_sequences=1,
        pad_token_id=tokenizer.eos_token_id
    )

    # Decode and return the generated response
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return generated_text


def generate_with_rag(query):
    """Generate a response from the LLM with document retrieval using FAISS."""
    # Retrieve the relevant document using FAISS
    query_embedding = embedder.encode([query])
    _, indices = faiss_index.search(np.array(query_embedding), k=1)
    relevant_document = documents[indices[0][0]]
    
    # Augment the query with the retrieved document
    augmented_prompt = f"Using the information from the document, answer the question.\n\nDocument: {relevant_document}\n\nQuestion: {query}\n\nAnswer:"
    
    # Tokenize the augmented prompt and generate response
    inputs = tokenizer(augmented_prompt, return_tensors="pt").to(device)

    outputs = model.generate(
        inputs['input_ids'],
        attention_mask=inputs['attention_mask'],
        max_length=50,
        num_return_sequences=1,
        pad_token_id=tokenizer.eos_token_id,
        no_repeat_ngram_size=3  # This prevents repeating the same n-grams
    )

    # Decode and return the generated response
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return generated_text

# Example usage
query = "Where is the Eiffel Tower located?"

# Generate response without RAG
response_without_rag = generate_without_rag(query)
print("Response WITHOUT RAG:")
print(response_without_rag)

# Generate response with RAG
response_with_rag = generate_with_rag(query)
print("\nResponse WITH RAG:")
print(response_with_rag)

```

    Response WITHOUT RAG:
    Where is the Eiffel Tower located?
    
    The Eiffel Tower is located in the city of Berlin, Germany. The tower is located in the city of Berlin, Germany. The tower is a part of the city of Berlin, Germany
    
    Response WITH RAG:
    Using the information from the document, answer the question.
    
    Document: The Eiffel Tower is located in Paris.
    
    Question: Where is the Eiffel Tower located?
    
    Answer: The Tower is in Paris, France.
    


```python
def download_pdf(url, output_path='manual.pdf'):
    response = requests.get(url)
    with open(output_path, 'wb') as file:
        file.write(response.content)
    print(f"PDF downloaded and saved as {output_path}")

def extract_text_from_pdf(pdf_path):
    reader = PyPDF2.PdfReader(pdf_path)
    text = ""
    for page in range(len(reader.pages)):
        text += reader.pages[page].extract_text()
    return text

# Download the PDF and extract text
url = "https://techinfo.honda.com/rjanisis/pubs/OM/AH/BTYA2222OM/enu/BTYA2222OM.pdf"
download_pdf(url, 'acura_mdx_manual.pdf')
manual_text = extract_text_from_pdf('acura_mdx_manual.pdf')
print("PDF text extracted.")

```

    PDF downloaded and saved as acura_mdx_manual.pdf
    PDF text extracted.
    


```python
def chunk_text(text, chunk_size=500):
    """Chunk the text into smaller pieces for embedding."""
    text_chunks = []
    words = text.split()
    for i in range(0, len(words), chunk_size):
        chunk = " ".join(words[i:i+chunk_size])
        text_chunks.append(chunk)
    return text_chunks

# Chunk the extracted text
text_chunks = chunk_text(manual_text)
print(f"Manual text chunked into {len(text_chunks)} chunks.")
```

    Manual text chunked into 273 chunks.
    


```python
# Load a pre-trained sentence transformer model
embedder = SentenceTransformer('all-MiniLM-L6-v2')

# Generate embeddings for each text chunk
chunk_embeddings = embedder.encode(text_chunks)

# Store embeddings using FAISS for efficient similarity search
dimension = chunk_embeddings.shape[1]  # Dimension of the embeddings
faiss_index = faiss.IndexFlatL2(dimension)  # L2 distance index
faiss_index.add(np.array(chunk_embeddings))

print("Embeddings created and indexed.")
```

    Embeddings created and indexed.
    


```python
def generate_response_with_llm(query, context):
    """Generate a response based on the relevant manual chunk using the LLM."""
    prompt = f"Using the information from the owner's manual section below, answer the question concisely:\n\nManual Section: {context}\n\nQuestion: {query}\n\nAnswer:"
    
    # Set pad_token to eos_token
    tokenizer.pad_token = tokenizer.eos_token

    # Tokenize the input and set attention_mask
    inputs = tokenizer(prompt, return_tensors="pt", padding=True).to(device)
    attention_mask = inputs['attention_mask']
    
    # Set the `pad_token_id` to `eos_token_id`
    pad_token_id = tokenizer.eos_token_id
    
    # Generate the response with limited new tokens, attention mask, and no_repeat_ngram_size to prevent repetitions
    outputs = model.generate(
        inputs['input_ids'], 
        attention_mask=attention_mask, 
        max_new_tokens=100,  # Control the number of new tokens generated
        pad_token_id=pad_token_id,
        no_repeat_ngram_size=3  # Prevent repetition of 3-grams
    )
    
    # Decode the response
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response
```


```python
def search_owners_manual(query, top_k=1):
    """Search for the most relevant chunk based on the query in the owner's manual."""
    query_embedding = embedder.encode([query])
    distances, indices = faiss_index.search(np.array(query_embedding), top_k)
    results = [text_chunks[idx] for idx in indices[0]]
    return results

# Use the existing function `generate_response_with_llm` to generate responses

def query_acura_manual(query):
    """Query the Acura MDX owner's manual using embeddings and generate a response with the LLM."""
    # Search the owner's manual for relevant chunks
    result_chunks = search_owners_manual(query)
    context = result_chunks[0] if result_chunks else "No relevant information found."
    
    # Generate a response using the existing LLM function
    response = generate_response_with_llm(query, context)
    return response

# Example query
user_query = "When does the Auto high beam mode turn ON?"
response = query_acura_manual(user_query)
print(f"Final Response: {response}")
```

    Final Response: Using the information from the owner's manual section below, answer the question concisely:
    
    Manual Section: beam: All of the following conditions must be met before the high beams turn on. ●Your vehicle speed is 25mph (40 km/h) or more. ●There are no preceding or oncoming vehicle with headlights or taillights turned on. ●There are few street lights on the road ahead.One of the following conditions must be met before the low beams turn on. ●Your vehicle speed is 15 mph (24 km/h) or less. ●There is a preceding or oncoming vehicle with headlights or taillights turned on. ●There are many street lights on the road ahead.1How to Use the Auto High-Beam In the following cases, th e auto high-beam system may not switch the head lights properly or the switching timing may be ch anged. In case of the automatic switching operati on does not fit for your driving habits, please swit ch the headlights manually. •The brightness of the lights from the preceding or oncoming vehicle is intense or poor. •Visibility is poor due to the weather (rain, snow, fog, windshield frost, etc.). •Surrounding light sources, such as street lights, electric billboards and traf fic lights are illuminating the road ahead. •The brightness level of th e road ahead constantly changes. •The road is bumpy or has many curves. •A vehicle suddenly appears in front of you, or a vehicle in front of you is not in the preceding or oncoming direction. •Your vehicle is tilted with a heavy load in the rear. •A traffic sign, mirror, or other reflective object ahead is reflecting strong light toward the vehicle. •The oncoming vehicle freq uently disappears under roadside trees or be hind median barriers. •The preceding or oncoming vehicle is a motorcycle, bicycle, mobility scooter, or other small vehicle. The auto high-beam system keeps the headlight low beam when: •Windshield wipers are op erating at a high speed. •The camera has detected dense fog.22 ACURA MDX-31TYA6011.book 179 ページ ２０２１年１２月１４日 火曜日 午前１０時４２分uuOperating the Switches Around the Steering Wheel uAuto High-Beam 180Controls You can turn the auto high-beam system off. If you want to turn the system off or on, set the power mode to ON, then carry out the following procedures while the vehicle is stationary. To turn the system off:With the light switch is in AUTO, pull the lever toward you and hold it for at least 40 seconds. After the auto high-beam indicator light blinks twice, release the lever. To turn the system on: With the light switch is in AUTO, pull the lever toward you and hold it for at least 30 seconds. After the auto high-beam indicator light blinks once, release the lever.■How to Turn Off the Auto High-Beam1How to Use the Auto High-Beam If the Some driver assist systems cannot operate: Camera temperature too high message appears: •Use the climate control system to cool down the interior and, if necessary, also use de froster mode with the airflow directed toward the camera. •Start driving the vehicle to lower the windshield temperature, which cool s down the area around the
    
    Question: When does the Auto high beam mode turn ON?
    
    Answer: When the auto low-beam mode turns ON, the auto High-beam switch is turned OFF.
    
    How to turn off the Auto Low-Beams When the Auto low-beams are turned ON, you can turn off Auto High Beam.
    1How To Turn Off Auto High Beams When The Auto High beam is turned ON.
    2How To turn off auto High Beam When the High beam turns ON. The Auto high-beam switch is not turned OFF, but
    
